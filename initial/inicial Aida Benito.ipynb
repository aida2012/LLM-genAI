{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">INSTANTIATION OF THE LLM MODEL AND THE EMBEDDING</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.environ[\"LANGCHAIN_TRACING_V2\"]\n",
    "LANGCHAIN_API_KEY = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature = 0,\n",
    "    streaming = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMBEDDINGS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "## Embedding Techinque of OPENAI\n",
    "embed_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "print(len(embed_model.embed_query('hola')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ THE DIRECTORY AND LOAD THE FILE\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "# read documents\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "dir=r'C:\\Users\\Aulamultimedia\\Documents\\practicas\\eguins2\\pdf_tesis/'\n",
    "doc=read_doc(dir)\n",
    "\n",
    "dir_cv=r'C:\\Users\\Aulamultimedia\\Documents\\practicas\\eguins2\\pdf_cv/'\n",
    "doc_cv=read_doc(dir_cv)\n",
    "\n",
    "total=doc+doc_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc\n",
    "\n",
    "documents=chunk_data(docs=total,chunk_size=3000, chunk_overlap=50)\n",
    "# documents_cv=chunk_data(docs=doc_cv,chunk_size=3000, chunk_overlap=50)\n",
    "\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">LOAD THE DOCUMENTS AND VECTORS TO PINESTORE DB</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONNECT WITH PINECONE DATABASE\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "#Connect to DB Pinecone\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "index_name = 'eguins'\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "  pc.delete_index(index_name)\n",
    "  print(\"index {} borrado\".format(index_name))\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    print(\"index creado con el nombre: {}\".format(index_name))\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=3072,  # dimensionality of text-embedding models/embedding-001\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "        )\n",
    "else:\n",
    "    print(\"el index con el nombre {} ya estaba creado\".format(index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPSERT THE VECTORS IN TO THE PINECONE DATABASE\n",
    "\n",
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "namespace = \"espacio\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=documents,\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model, \n",
    "    namespace=namespace\n",
    ")\n",
    "print(\"upserted values to {} index\".format(index_name))\n",
    "\n",
    "time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">RETRIEVE AND SEARCH INTO THE CREATED PINECONE DATABASES</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = 'eguins'\n",
    "index_name_cv = 'eguinscv'\n",
    "namespace = \"espacio\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embed_model,\n",
    "    namespace=namespace,\n",
    ")\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "vectorstore_cv = PineconeVectorStore(\n",
    "    index_name=index_name_cv,\n",
    "    embedding=embed_model,\n",
    "    namespace=namespace,\n",
    ")\n",
    "retriever_cv=vectorstore_cv.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"in which companies did ezequiel used to work\"\n",
    "vectorstore.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA  \n",
    "\n",
    "query = \"does Ezequiel have any hands-on experience\"\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "qa_cv = RetrievalQA.from_chain_type(  \n",
    "    llm=chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore_cv.as_retriever()  \n",
    ") \n",
    "result = qa_cv.invoke(query)\n",
    "\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empieza lo de aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Cargar API KEY de PINECONE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar las variables del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Acceder a las claves\n",
    "PINECONE_API_KEY= os.getenv(\"BD_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anben\\Desktop\\posgrado\\LLM\\llm_env\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# 2- Impmortar las librerias necesarias\n",
    "from pinecone import Pinecone,ServerlessSpec,Index\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 2 fragmentos: [Document(page_content='AÍDABENITO\\nanbenito@yahoo.com.ar\\nQuilmes, BuenosAires, Argentina\\nCell: (54911)6-497-6858-Skype: anbenito1975@gmail.com\\nLinkedIn:https://www.linkedin.com/in/aida-benito/\\nINFORMATIONSYSTEMSENGINEER\\nExperiencedDataEngineer andBusiness IntelligenceConsultant. Possessesadvancedknowledgeof\\ndatabases and various BI tools, as well as knowledge of programming languages, AWS and GCP\\nEcosystems. Has strong communication skills. Passionate about Data Science, BigData, Internet of', metadata={'source': 'C:\\\\Users\\\\anben\\\\Desktop\\\\resume_Aida Benito.pdf', 'page': 0}), Document(page_content='Things, Artificial Intelligence, research and learning. Able to handle several projects simultaneously.\\nHaspassedtheFirst CertificateinEnglishexam.\\nCorecompetenciesinclude:\\nFlexibility-Leadership-Teamwork-Conflict resolution-Analytical thinking-Decisionmaking\\nCustomerService-Creativethinking\\nPROFESSIONALEXPERIENCE\\nMPSGroup-Client: DeltaAirlines\\n● Designandproposearchitecturesolutions\\n● DevelopETLcomponents\\n● DevelopSQLqueries', metadata={'source': 'C:\\\\Users\\\\anben\\\\Desktop\\\\resume_Aida Benito.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "# 3- Cargar el archivo PDF\n",
    "pdf_location = r'C:\\Users\\anben\\Desktop\\resume_Aida Benito.pdf'\n",
    "loader = PyPDFLoader(pdf_location)\n",
    "documents = loader.load()\n",
    "\n",
    "# Dividir el documento en fragmentos pequeños\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Ver los primeros fragmentos para asegurar que la división fue correcta\n",
    "print(f\"Primeros 2 fragmentos: {split_documents[:2]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index resume-index borrado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4- Inicializar Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)\n",
    "cloud = 'aws'\n",
    "region = 'us-east-1'\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "\n",
    "\n",
    "index_name = \"resume-index\"\n",
    "if index_name in pc.list_indexes().names():\n",
    "  pc.delete_index(index_name)\n",
    "  print(\"index {} borrado\".format(index_name))\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Si el índice no existe, lo creamos\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  \n",
    "        metric='cosine',\n",
    "        spec=spec)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados y almacenados en el índice 'resume-index' de Pinecone.\n"
     ]
    }
   ],
   "source": [
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generar embeddings usando el modelo \n",
    "texts = [doc.page_content for doc in split_documents]\n",
    "embeddings = embed_model.encode(texts)\n",
    "\n",
    "# Preparar los datos para subirlos a Pinecone\n",
    "#upsert_data = [(str(i), embeddings[i], {}) for i in range(len(embeddings))]\n",
    "\n",
    "# Subir los embeddings al índice\n",
    "#index.upsert(vectors=upsert_data)\n",
    "upsert_data_generator = [\n",
    "    {\n",
    "        'id': str(i), \n",
    "        'values': embedding, \n",
    "        'metadata': {'text': texts[i]}  # Los metadatos (por ejemplo, texto del documento)\n",
    "    }\n",
    "    for i, embedding in enumerate(embeddings)\n",
    "]\n",
    "\n",
    "index.upsert(vectors=upsert_data_generator)\n",
    "\n",
    "print(f\"Embeddings generados y almacenados en el índice '{index_name}' de Pinecone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
